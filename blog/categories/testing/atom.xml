<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Testing | Mike Murray]]></title>
  <link href="http://mkmurray.com/blog/categories/testing/atom.xml" rel="self"/>
  <link href="http://mkmurray.com/"/>
  <updated>2012-12-15T15:41:41-07:00</updated>
  <id>http://mkmurray.com/</id>
  <author>
    <name><![CDATA[Mike Murray]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Value is the Boundary]]></title>
    <link href="http://mkmurray.com/blog/2012/12/15/the-value-is-the-boundary/"/>
    <updated>2012-12-15T15:31:00-07:00</updated>
    <id>http://mkmurray.com/blog/2012/12/15/the-value-is-the-boundary</id>
    <content type="html"><![CDATA[<p><em>Author's Note: I do not take credit for the phrase used as the title of this
blog post. It comes from Gary's Ruby Conf 12 video recording entitled
<u>Boundaries</u> mentioned below and in the previous blog post.</em></p>

<p>In the last blog post <a href="/blog/2012/12/08/testing-trade-offs/">Testing
Trade-offs</a>, I talked about the
difficulties of verifying the decisions and dependencies of our classes with the
current mainstream testing methodologies. Based on a recorded conference talk by
<a href="https://twitter.com/garybernhardt">Gary Bernhardt</a>, the focus was on
effectively testing the logic a class contains and the dependent collobrators it
takes in for coordinating with other classes and objects to perform its
responsibilities. Mixing the two concerns in the same object definition requires
utilizing both isolated unit testing and integration/integrated testing in order
to adequately test cover the class. However, code designed this way seems to
play to the weaknesses of each testing strategy just as much as it plays to
their strengths (please see the previous blog post if you would like more
details on that discussion).</p>

<p>Today let's go one step further and talk about ways that we could more cleanly
separate the concerns of decisions and dependencies, with the hope that we can
create objects that better lend themselves to one type of testing over the
other. Gary proposes that a such a codebase could have better modularity,
scalability, and even concurrency. I assert that your code will also be more
maintainable and extensible as well. Most of today's content will take a lesson
from the functional programming paradigm, including practices they have espoused
for decades.</p>

<h2>Frictionless Isolated Unit Testing</h2>

<p>If you wanted to test the mathematical addition operator (i.e., the plus sign
<code>+</code>), what frameworks, tools, and/or coding tricks do you have to employ to
sufficiently isolate it from all other concerns and objects? Absolutely nothing!
There are no dependencies to mock or stub; it isolates for free. Why is that?
Gary cautions against assuming it is because the addition operator is simple and
lacks complexity. He digs deeper in order to identify two properities the
implementation of plus sign exhibits that allows it to be naturally isolated.</p>

<p>The first property is that the operator takes values as arguments and returns
new values as output without any mutation. The second property is that the
operator requires no dependencies in order to perform its computation and logic.
Thus there is nothing to mock or stub when testing it, which was the major
weakness of isolated unit testing. Also because of the lack of dependencies, no
integrated tests are required in order to better test how the operator will
behave in a production environment where there are no mocks and stubs. To test
the addition operator we merely need to write simple pass-values-in,
assert-value-out tests with no extra setup required.</p>

<p>As Gary applies these concepts to existing code, we notice a few changes. Pieces
of domain logic and pieces of code that coordinate dependencies are separated
from each other, broken out into new objects created for a single purpose and
responsibility. The nature of the communication between objects also changed,
with values (inputs and outputs) becoming the boundary between objects instead
of the emphasis being on several synchronous method calls. Value objects focused
on data (and not behavior) become the new contract between collaborating
classes.</p>

<h2>To Be Continued...</h2>

<p>You may notice that many of these concepts have a functional programming
influence. The properties of immutability and focus on data at the boundaries
allow us to write code that isolates very easily and lends itself very well to
isolated unit testing that is simple and not brittle. It is very good at
verifying the domain logic and decision paths of our objects. Of course, we
can't write the entirety of our codebase in this manner with no dependencies
ever. Next time I will discuss a code architecture that Gary proposes which can
utilize this style of code married with some more imperative glue code that
coordinates the dependencies in the system. We will also discuss testing those
portions of code as well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Trade-offs]]></title>
    <link href="http://mkmurray.com/blog/2012/12/08/testing-trade-offs/"/>
    <updated>2012-12-08T21:11:00-07:00</updated>
    <id>http://mkmurray.com/blog/2012/12/08/testing-trade-offs</id>
    <content type="html"><![CDATA[<p>Last week our dev team at Extend Health watched a <a href="http://www.youtube.com/watch?v=yTkzNHF6rMs">RubyConf 2012 video of a talk
entitled Boundaries</a> by <a href="https://twitter.com/garybernhardt">Gary
Bernhardt</a> of <a href="https://www.destroyallsoftware.com/screencasts">Destroy All
Software</a> (or perhaps even
better known for the infamous <a href="https://www.destroyallsoftware.com/talks/wat">WAT lightning talk at CodeMash
2012</a>). Gary proposes a very
interesting code architecture that marries the individual benefits of
immutability &amp; mutability, functional programming &amp; imperative object-oriented
programming, isolated unit testing &amp; integration testing. He discusses the pros
&amp; cons of each code design &amp; testing decision and the trade-offs that we end up
dealing with. He suggests a potential solution that makes virtually no trade-off
and attempts to harness the advantages of each of these methodologies that
appear to be at odds with one another. I feel the idea has a lot of merit and I
highly encourage everyone to watch his presentation to get the full context and
a better logical progression of his proposal than what I can provide.</p>

<h2>Isolated Unit Testing vs. Integration Testing</h2>

<p>A common practice in unit testing classes and objects is to isolate the targeted
class from its dependencies so that you can focus solely on its responsibilities
and domain logic independent of any implementation details of the dependencies.
This is typically accomplished by the use of stubs and mocks, which attempt to
control and monitor the interactions with and data return from dependencies.
There are a ton of gains afforded by this style of testing as Gary points out,
but one very large criticism of this testing methodology is that it doesn't
exercise the code in the same way it would run in production. It is not exactly
rare to have these isolated unit tests successfully pass and yet still have
problems in production.</p>

<p>Integration testing tends to better emulate production because it maintains the
interaction relationships between objects in addition to acquiring and passing
the data around the system in the same way. The criticism Gary puts forth of an
integrated testing strategy is that it is very slow as you begin to attempt to
cover all the code paths through the system. Consider trying to cover all
logical branches through the system, including all branches of
try/catch/finally, conditional, and looping structures. Gary suggests the growth
in code to cover these scenarios is 2<sup>n<sup>,</sup></sup> where <code>n</code> is the number of branches.</p>

<p>Many try to get the benefits of both types of testing in order to compensate for
each strategy's shortcomings. However, they still write code that doesn't play
to each testing methodolgy's strengths. The strength of isolated unit testing is
verifying that given certain inputs the expected output is always returned. The
strength of integration testing is coordinating dependencies, making sure they
utilize and interact with the API of other objects correctly. If you design your
classes to be a mix of dependencies and logic, it becomes difficult to
effectively test cover them without writing both sets of tests for all classes
in your codebase.</p>

<h2>So What?</h2>

<p>OK, so you are probably wondering what the point is then. This sounds like a
good plan to just be more disciplined in your test coverage, right? Well, let's
explore a way to better segregate dependency orchestration from actual logic and
behavior. This could allow us to use the right testing methodology depending on
which type of responsibility the object is meant to encapsulate: dependencies or
decisions? Gary also asserts that it will lead us down a path that could yield a
codebase with better modularity, scalability, and potentially even concurrency.
I believe you can also add better maintainability and extensibility to that
list.</p>

<p>Next week I will dive into how Gary suggests we can better seperate these
concerns of dependencies and decisions. Stay tuned (or go watch the video and
spoil the surprise).</p>

<p>Next post: <a href="/blog/2012/12/15/the-value-is-the-boundary/">The Value is the Boundary</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Refactoring Legacy Code Starts with Test Coverage]]></title>
    <link href="http://mkmurray.com/blog/2012/11/10/refactoring-legacy-code-starts-with-test-coverage/"/>
    <updated>2012-11-10T15:37:00-07:00</updated>
    <id>http://mkmurray.com/blog/2012/11/10/refactoring-legacy-code-starts-with-test-coverage</id>
    <content type="html"><![CDATA[<p>As a dev team, we have been watching and discussing various training videos and
conference talks every morning for a few months now. This week we watched a
recorded conference talk by Ruby developer <a href="http://kytrinyx.com/">Katrina Owen</a>
entitled <a href="http://www.youtube.com/watch?v=J4dlF0kcThQ">Therapeutic Refactoring</a>.
I was especially impressed with her strategy for refactoring legacy code.</p>

<p>I'm sure everyone can agree that the safest way to refactor code and be 100%
positive that the functionality hasn't regressed is to have a good suite of
tests. And if you don't have sufficient test coverage (or any at all), then now
is the right time to add it before refactoring.</p>

<p>However, I have found myself skipping this step in the past, feeling justified
because the quality of the code was so poor or the deadline was so short. It is
irresponsible of me to assume that I have not changed the functionality of the
code without proof in the form of passing tests that sufficiently cover all
scenarios and use cases.  Nonetheless, watching Katrina's presentation has
given me renewed commitment and a sound plan of attack against legacy code that
may be ugly and very unfamiliar to me.</p>

<p>Here is the example method that Katrina uses during her presentation (which is
later revealed as a piece of code that she vaguely remembers coding herself and
now abhors):</p>

<pre class="brush: ruby">
    module XYZService

      def self.xyz_filename(target)
        # File format:
        # [day of month zero-padded][three-letter prefix] \
        # _[kind]_[age_if_kind_personal]_[target.id] \
        # _[8 random chars]_[10 first chars of title].jpg
        filename = "#{target.publish_on.strftime("%d")}"
        filename << "#{target.xyz_category_prefix}"
        filename << "#{target.kind.gsub("_", "")}"
        filename << "_%03d" % (target.age || 0) if target.personal?
        filename << "_#{target.id.to_s}"
        filename << "_#{Digest::SHA1.hexdigest(rand(10000).to_s)[0,8]}"
        truncated_title = target.title.gsub(/[^\[a-z\]]/i, '').downcase
        truncate_to = truncated_title.length > 9 ? 9 : truncated_title.length
        filename << "_#{truncated_title[0..(truncate_to)]}"
        filename << ".jpg"
        return filename
      end

    end
</pre>


<p>It certainly isn't the worst lines of code ever written and it is getting the
job done, but it is definitely not very readable and quite difficult to
understand its purpose and logic. About the only thing that can be discerned is
that it is intended to generate a filename in a specific format.</p>

<p>Katrina starts by writing a very simple and seemingly useless test (which she
calls a "Mickey Mouse test" in her GitHub commit message) in order to begin test
coverage of this method:</p>

<pre class="brush: ruby">
    require_relative './xyz_service'

    describe XYZService do

      let(:target) do
        stub(:target)
      end

      subject { XYZService.xyz_filename(target) }

      it 'works' do
        subject.should eq('something')
      end

    end
</pre>


<p>Without much knowledge of what this method's purpose or inputs are, we can only
pass in an empty stub object as input and then assert something rediculous for
the result. We expect this test to not only fail to pass, but it is also likely
to error during runtime (or even at compile time if in a statically typed
language like C#). The idea is to create a quick feedback loop that aids us in
finding the context and inputs required to make the code execute without runtime
error. Then we can finally see the actual return value and modify our dummy
assertion to begin expecting that output.</p>

<p>After iterating in this fashion, Katrina arrives at the following test which is
a bit more useful:</p>

<pre class="brush: ruby">
    require_relative './xyz_service'
    require 'date'

    describe XYZService do

      let(:target) do
        messages = {
          :publish_on => Date.new(2012, 3, 14),
          :xyz_category_prefix => 'abc',
          :kind => 'unicorn',
          :personal? => false,
          :id => 1337,
          :title => 'magic & superglue'
        }
        stub(:target, messages)
      end

      subject { XYZService.xyz_filename(target) }

      it 'works' do
        subject.should eq('14abcunicorn_1337_cb6c53bc_magicsuper.jpg')
      end

    end
</pre>


<p>However, this test doesn't pass because the <code>cb6c53bc</code> portion of the resultant
filename is different every execution of the test. A simple regex allows the
test to pass all of the time:</p>

<pre class="brush: ruby">
    subject.should match(/14abcunicorn_1337_[0-9a-f]{8}_magicsuper\.jpg/)
</pre>


<p>Even though we now have a passing test, we have not yet exercised the different
input values and all code paths and branches through the method. This must be
done in order to fully understand, document, and test cover the logic and
purpose of the method. This would be done in the normal unit testing fashion
that is shown in just about every testing demo by adding more test scenarios and
assertions that excercise the varying use cases.</p>

<p>One interesting development that occurred while Katrina was using this process
was that she actually found a bug in the implementation of the original
<code>xyz_filename</code> method, specifically an extra set of braces in the regex on line
14.</p>

<p>Now that we are fully covered with tests (and a bug fixed), we are free to
refactor the code into smaller, more readable chunks of code logic that are more
cohesive and more reusable as well. This is what Katrina does for the remainder
of her presentation, and it is just as instructive as this first part. However,
today I wanted to focus on her strategy for writing test coverage against a
legacy codebase.</p>

<p>I like how what Katrina is demonstrating seems to utilize a black box testing
approach at first, and then progresses to a more white box approach as we begin
to explore the different parts of the implementation. Perhaps such a strategy
should have been obvious to me, but Katrina's conference talk really struck a
chord with me and encouraged me to commit to doing better with test coverage in
legacy codebases before I begin making modifications or additions.</p>

<p>I highly recommend viewing the entire video, as it is only 30 minutes long and
an entertaining watch.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Anagram Code Kata Part 5 – Domain Objects Over Language Primitives]]></title>
    <link href="http://mkmurray.com/blog/2010/02/20/anagram-code-kata-part-5-domain-objects-over-language-primitives/"/>
    <updated>2010-02-20T17:55:00-07:00</updated>
    <id>http://mkmurray.com/blog/2010/02/20/anagram-code-kata-part-5-domain-objects-over-language-primitives</id>
    <content type="html"><![CDATA[<div class='post'>
<p>This post is part of a <a href="http://murrayon.net/2009/11/anagram-code-kata-bdd-mspec.html">series on coding Kata, BDD, MSpec, and SOLID principles</a>.&#160; Feel free to visit the above link which points to the introductory post, also containing an index of all posts in the series.</p>  <p>In this post, we will discuss some reasons why you might want to avoid using language primitives directly in place of domain objects.&#160; Specifically, I have been using <font face="Courier New">String</font> variables and objects to represent words up to this point.&#160; <a href="http://adotnetdude.blogspot.com/">Esteban</a> suggested I create a <font face="Courier New">Word</font> class for a few good reasons which I’ll layout for you.&#160; Out of sheer luck and good timing, <a href="http://codebetter.com/blogs/dru.sellers/default.aspx">Dru Sellers</a> of <a href="http://codebetter.com/">CodeBetter.com</a> also wrote on this subject shortly afterward and confirmed Esteban’s reasoning.</p> <span class="fullpost">   <h3>Use <font face="Courier New">String</font> Class or Create <font face="Courier New">Word</font> Domain Object?</h3>    <p>I mentioned in the previous post that Esteban had suggested creating a <font face="Courier New">Word</font> class instead of just passing around strings everywhere through my application.&#160; One good reason is that we don’t own the <font face="Courier New">String</font> class and so further modification to our string-based implementation is difficult because the logic is scattered throughout the application, instead of centralized under the responsibility and definition of one domain class.&#160; It is much more difficult to refactor the word-specific logic with its behaviors and attributes spread throughout the code.&#160; Even if your <font face="Courier New">Word</font> class doesn’t grow any more beyond a seemingly unnecessary wrapper of the <font face="Courier New">String</font> class, the code is more cohesive and ready for change should the need arise.</p>    <p>But more importantly, you designed the code thinking in a true object-oriented mindset.&#160; You have to keep in mind that the <font face="Courier New">String</font> class is someone else’s implementation, and is hardly ever sufficient in and of itself as a domain object within your solution.&#160; Think about it, the behaviors that a string object performs are so generic and multipurpose that you likely don’t need two-thirds of the class as defined (and there’s likely a few behaviors you really need that aren’t there).&#160; Of course nearly every application on earth makes use of the <font face="Courier New">String</font> class; but because of this fact, it has no meaning in and of itself within any given application.&#160; You would have to look at how the stings are actually used within the code in order to understand its unique application within the app’s context.&#160; Of course that task of research is much easier for everyone (including the original author of the code) if it’s all encapsulated within a dedicated domain object class.&#160; True object-orientation means describing in code form the properties, behaviors, and interactions/relationships of real world objects within your problem domain.</p>    <p>Esteban gave me a great example to illustrate these points.&#160; He said that you can always represent money as a decimal (and even when you use a domain object, it’s got to have a decimal language primitive underneath the covers).&#160; However, what happens when you need to attach metadata to the amount (like currency denomination), or if you need to change decimal precision?&#160; You would have to go through all of the code and make sure your use of decimal language primitives is modified uniformly in order to retain consistency.&#160; Also, mathematic operations involving money are hardly ever the same as their counterparts involving standard decimals, because currency deals with discrete values to a certain decimal precision.&#160; Typically when the behaviors and properties within our system begin to get complex, we are cognizant enough to create domain objects in order to bring it all under one class.&#160; We definitely don’t want to over-architect features and interactions before we need them, but I think there is power in this principle of abstracting away language primitives and instead encapsulating their use within domain objects located in just one place in your codebase.&#160; I believe it is one thing we can keep in mind to help guide us to better object-oriented thinking, and avoid language-oriented coding.</p>    <p>As mentioned above, Esteban’s thoughts were confirmed nearly verbatim by <a href="http://codebetter.com/blogs/dru.sellers/default.aspx">Dru Sellers</a> of <a href="http://codebetter.com/">CodeBetter.com</a> in his blog post that he wrote just a few days after I had the conversation with Esteban.&#160; A great coincidence no doubt, and worth a read; here’s the link:</p>    <p><a href="http://codebetter.com/blogs/dru.sellers/archive/2010/01/27/business-primitives-1-2.aspx">Business Primitives (1/2)</a>&#160;</p>    <h3><font face="Courier New">Word</font> Class Implementation</h3>    <p>So basically I created the following class implementation and then replaced string with a reference to this new class, <font face="Courier new">Word</font>:</p>    <pre class="brush: csharp">public class Word
{
    private string wordStr = string.Empty;

    public Word(string wordStr)
    {
        this.wordStr = wordStr;
    }

    public override string ToString()
    {
        return wordStr;
    }

    public override bool Equals(object obj)
    {
        return wordStr == ((Word)obj).ToString();
    }

    public override int GetHashCode()
    {
        return base.GetHashCode();
    }

    public int GetCanonicalHashCode()
    {
        char[] letters = wordStr.ToCharArray();
        Array.Sort&lt;char&gt;(letters);
        return new string(letters).GetHashCode();
    }
}</pre>

  <p>I have defined an overridden implementation for <font face="Courier New">Equals(object)</font> (so that the test assertions and other <font face="Courier New">IEnumerable.Contains()</font> queries work) and <font face="Courier New">GetHashCode()</font> (solely to satisfy a compiler warning).&#160; I also moved the <font face="Courier New">GetCanoncialHashCode()</font> method from <font face="Courier New">AnagramGrouper</font>, in order to better encapsulate it as a behavior a Word knows how to do innately.</p>

  <p>One other change I had to make was to convert strings into <font face="Courier New">Word</font> objects in our <font face="Courier New">NewlineFileParser</font>, which I accomplished by using an <font face="Courier New">IEnumberable.Select()</font> call as shown below:</p>

  <pre class="brush: csharp">return File.ReadAllLines(filePath).Select&lt;string, Word&gt;(x =&gt; new Word(x));</pre>

  <p></p>

  <h3>Let’s Revisit <font face="Courier New">AssertWasCalled</font> One More Time</h3>

  <p>Trust me, I am groaning with you, even as I wrote that heading text.&#160; The last thing we need is for me to rehash the topic again and flip flop my stance yet another time.&#160; Yes, that’s right I’ve changed my mind again.&#160; First I couldn’t understand what utility asserting methods were called would have under normal test scenarios.&#160; Then I changed my mind that perhaps using it would help my test specifications have clearer intent of what I am asserting.&#160; After an email conversation with <a href="http://www.davesquared.net/">David Tchepak</a>, I think I’m now back to my original stance.&#160; Here is what Dave said that had me reconsidering, and I think it’s pretty sound reasoning (emphasis added by me):</p>

  <blockquote>
    <p>…</p>

    <p>“But seeing as you asked for it, here goes. :)</p>

    <p>“I try and avoid <font face="Courier New">AssertWasCalled</font> like the plague. Generally I don't care that some method was called, I care that my class does the work it needs to. If that involves calling a dependency then great, but that is not my class' reason for existence.</p>

    <p>“I prefer your original approach of stubbing out everything in the setup and having that tested indirectly. One reason I prefer this is I find it makes it easier to refactor: the assertions in my test don't change, the class still does the same thing. However I can add or change dependencies by changing some wiring in the setup, and then make sure I haven't stuffed anything up as my assertions still pass. <strong>I prefer that my tests specify what I want, not how it does it.</strong> To me, <font face="Courier New">AssertWasCalled</font> reeks of over-specification. The one exception is where I hit the external boundaries of my code, so where I want to send an email or something without side effects that I can test. Then the core of the behaviour is the call itself, so I'm happy to assert on that then.”</p>
  </blockquote>

  <p>And with that I’ll promise to never bring this up again…unless of course I get swayed by someone else. :)&#160; In all seriousness, I think this is an interesting discussion and so if you have insight, please share via comment below.</p>

  <h3>Summary</h3>

  <p>Please leave your thoughts in the comments below in regard to creating domain objects over using language primitives (or heaven forbid, the <font face="Courier New">AssertWasCalled</font> debate).&#160; As far as this coding kata exercise, I need to take a high-level look at where this should go next.&#160; Perhaps the next post will tie up loose ends and see how our code performs on large text files as input.&#160; It may be that we will need to refactor our architecture to achieve better runtimes.&#160; If not, maybe we can still discuss where would could have headed if it had been necessary.</p>
</span>  </div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Anagram Code Kata Part 4 – Will it Write My Code for Me?]]></title>
    <link href="http://mkmurray.com/blog/2010/01/26/anagram-code-kata-part-4-will-it-write-my-code-for-me/"/>
    <updated>2010-01-26T22:27:00-07:00</updated>
    <id>http://mkmurray.com/blog/2010/01/26/anagram-code-kata-part-4-will-it-write-my-code-for-me</id>
    <content type="html"><![CDATA[<div class='post'>
<p>This post is part of a <a href="http://murrayon.net/2009/11/anagram-code-kata-bdd-mspec.html">series on coding Kata, BDD, MSpec, and SOLID principles</a>.&#160; Feel free to visit the above link which points to the introductory post, also containing an index of all posts in the series.</p>  <p>In this post, we work on implementing the other main dependency, <font face="Courier New">IAnagramsFinder</font>.&#160; In doing so, you will discover that I have been a little naïve with this BDD methodology. Apparently I expected a different experience than what I ran into doing the exercise.&#160; In gathering feedback, I had a very interesting discussion about Object-Oriented Programming with <a href="http://adotnetdude.blogspot.com/">Esteban</a> that I will summarize and ask for further discussion from all of you.&#160; I’ll also show you one way to debug your code when running the MSpec tests via the console test runner.</p> <span class="fullpost">   <h3>New Thoughts on Mocks vs. Stubs</h3>    <p>During <a href="http://murrayon.net/2009/11/anagram-code-kata-part-2-mocking-and.html">Part 2 of this series</a>, <a href="http://papamufflon.blogspot.com/">Tobias</a> left <a href="http://murrayon.net/2009/11/anagram-code-kata-part-2-mocking-and.html#comments">a comment about using mocks and <font face="Courier New">AssertWasCalled</font></a>, rather than stubs and defining fake responses.&#160; I didn’t understand how it would help, as it appeared they would accomplish roughly the same thing.&#160; With stubs and fake responses, if you don’t get back the return you are looking for, then you can infer that the dependencies weren’t called since you had previously hard-coded their return values when called.&#160; With mocks and asserting methods were called, you directly assert that the manager class did in fact call it’s dependencies.</p>    <p>When I looked more closely into using the <font face="Courier New">AssertWasCalled</font> method in Rhino.Mocks, I noticed that my strategy of using stubs and faked return values wasn’t very explicit in signifying that the ultimate responsibility of the manager class was to delegate to its dependencies.&#160; I mean the test context does rigorously stub out the dependencies, but the actual assertions weren’t clear what we were testing.&#160; Therefore, I decided to pop in a pair of <font face="Courier New">AssertWasCalled</font> statements (as found below), leaving <a href="http://murrayon.net/2009/11/anagram-code-kata-part-2-mocking-and.html#firstSpec">the rest of the test the same as it was back in Part 2</a>.</p>    <pre class="brush: csharp">It should_result_in_list_of_anagram_sets_and_both_dependencies_should_have_been_called = () =&gt;
{
    fileParser.AssertWasCalled(x =&gt; x.ExtractWordListFromFile(filePath));
    anagramGrouper.AssertWasCalled(x =&gt; x.FindAnagramSets(wordListFromFile));

    result.ShouldEqual(expected);
};</pre>

  <p>However, when I thought to simplify the test code by removing most of stubbing setup, it appeared I had to leave most of the stub logic in tact in order to test if <font face="Courier New">anagramGrouper.FindAnagramSets()</font> was called with parameter <font face="Courier New">wordListFromFile</font>, which was one of the stubbed fake responses.&#160; The one dependency (the file parser) needed to pass it’s return value to the next dependency (the anagrams finder).&#160; I could not think of a way to accomplish testing that the manager class facilitated that without the stubbing logic (fake responses) that I created.</p>

  <p>It would be great if someone could enlighten me since I’m such a newbie to mocking and stubbing.&#160; Otherwise, I feel the intent of the test is now more clear with the <font face="Courier New">AssertWasCalled</font> checks that I added, even if that I means I’m needlessly mixing stubbing and mocking.&#160; But one last question:&#160; should I try to stick more closely to the rule of thumb guiding me to stick to one all-encompassing assertion per test if possible?&#160; Let me know your thoughts.</p>

  <h3>Implementing the <font face="Courier New">AnagramGrouper</font> Dependency</h3>

  <p>So let’s continue onward with some more actual design and code.&#160; We are tackling the <font face="Courier New">AnagramGrouper</font> dependency, whose sole responsibility is to take in the parsed collection of words and find anagram sets.&#160; Here is the test spec (one thing to note is that in some of my earlier tests for the <font face="Courier New">AnagramsFinder</font> manager class and <font face="Courier New">IFileParser</font>, I had used string arrays liberally; I decided to go with <font face="Courier New">IList&lt;string&gt;</font> collections instead):</p>

  <pre class="brush: csharp">public class AnagramGrouperSpecs
{
    [Subject(typeof(AnagramGrouperSpecs))]
    public class when_given_word_list_collection
    {
        static AnagramGrouper sut;
        static IEnumerable&lt;IList&lt;string&gt;&gt; result;
        static IEnumerable&lt;string&gt; wordList = new[] { &quot;wordA&quot;, &quot;wordB&quot;, &quot;Aword&quot;, &quot;wordC&quot; };

        Establish context = () =&gt;
        {
            sut = new AnagramGrouper();
        };

        Because of = () =&gt;
        {
            result = sut.FindAnagramSets(wordList);
        };

        It should_result_in_anagram_sets_collection_of_length_1 = () =&gt;
        {
            result.Count().ShouldEqual(1);
        };

        It should_contain_2_specific_words_in_the_anagram_set = () =&gt;
        {
            result.First().ShouldContain(&quot;wordA&quot;, &quot;Aword&quot;);
        };
    }
}</pre>

  <p>Here too I have basically two assertions (but split it up differently than last time); should I also try harder to avoid this as well?</p>

  <p>When contemplating how to further breakdown this responsibility of finding anagram sets into subtasks, I figure we would need one piece of code that would compare two words and answer whether they are anagrams of each other, and another piece of code that would keep track of the various anagram sets (perhaps a collection or dictionary of string arrays).&#160; With what BDD has revealed to me thus far, I figure this means we are going to see two more dependencies (and interfaces, and spec tests) added to the codebase.</p>

  <p>So I start with the word comparison task, thinking I could perhaps leverage the <font face="Courier New">String.GetHashCode()</font> method for assigning meaningful value to the list of characters in the string.&#160; However, string hash codes give importance to letter casing and order of characters, so a modified strategy would have to be utilized, even though <font face="Courier New">GetHashCode()</font> seems quite close to filling our need.</p>

  <p>Now I probably should have come up with this ingenious algorithm all by myself, but…it is what it is and this is the best, most concise solution that I can put together to solve the problem.&#160; With that said, I ended up doing some internet searching in order to proof-of-concept my <font face="Courier New">GetHashCode()</font> idea.&#160; I came across pure genius in the form of <a href="http://www.formatexception.com/">Brian Mullen’s</a> blog post entitled <a href="http://www.formatexception.com/2009/03/linq-group-by-and-groupby/">LINQ group by and GroupBy</a>, specifically the second-to-last code snippet.&#160; Just <font face="Courier New">String.ToCharArray()</font> the word and then <font face="Courier New">Array.Sort()</font> the resultant array; now turn it back into a string and do <font face="Courier New">GetHashCode()</font> now.&#160; (As a side note, we can worry about case-insensitivity later if desired by simply doing a <font face="Courier New">String.ToLower()</font> on the word before getting the hash code.)&#160; Now anagrams will have the same hash code (Brian uses the term “canonical”, which I think is very fitting); it’s literally perfect in every way!</p>

  <p>So now for the other dependency, I figure we could use a <font face="Courier New">Dictionary&lt;int, IList&lt;string&gt;&gt;</font>, where the key is the canonical hash code and the value is a string list collection representing an anagram set of words.&#160; These two dependency implementations are so simple and straight forward that I have a hard time seeing a reason to continue breaking out full-fledged object dependencies with interfaces and Dependency Injection.&#160; However, I will make a case for moving the canonical hash code logic outside this <font face="Courier New">AnagramsGrouper</font> class, but that will come later.</p>

  <p>So let’s write some code to pass our test/spec:</p>

  <pre class="brush: csharp">public class AnagramGrouper : IAnagramGrouper
{
    public IEnumerable&lt;IList&lt;string&gt;&gt; FindAnagramSets(IEnumerable&lt;string&gt; wordList)
    {
        Dictionary&lt;int, IList&lt;string&gt;&gt; results = new Dictionary&lt;int, IList&lt;string&gt;&gt;();

        foreach (string word in wordList)
        {
            int canonicalHashCode = GetCanonicalHashCode(word);

            if (results.ContainsKey(canonicalHashCode))
            {
                results[canonicalHashCode].Add(word);
            }
            else
            {
                results.Add(canonicalHashCode, new List&lt;string&gt;(new[] { word }));
            }
        }

        return results.Values;
    }

    public static int GetCanonicalHashCode(string word)
    {
        char[] letters = word.ToCharArray();
        Array.Sort&lt;char&gt;(letters);

        return new string(letters).GetHashCode();
    }
}</pre>

  <p>Maybe you noticed already, but there is a bug in my logic.&#160; At that time, I didn’t see it and needed to debug the code while running.&#160; Setting a debug break point in your code doesn’t work like you’re probably used to, and this is because we’re running the MSpec tests via its console test runner.&#160; I had to do some searching to figure this one out, and that’s what we’ll cover next.</p>

  <h3>How to Debug MSpec Specifications and the Code They Test</h3>

  <p>After a short ride on the Google super highway, I found a <a href="http://codebetter.com/blogs/aaron.jensen/archive/2008/09/02/mspec-v0-2.aspx#comments">blog post containing a November 2009 comment</a> from <a href="http://codebetter.com/blogs/aaron.jensen/default.aspx">Aaron Jensen</a>, author of MSpec, that gave a few possible solutions.&#160; I opted to go with the <font face="Courier new">Debugger.Break()</font> solution, which I put at the very top of my <font face="Courier new">FindAnagramSets</font> method.&#160; It is a tad awkward because it will show a Windows error dialog box that says the program encountered a problem and Windows is searching the web for a solution.&#160; If you wait through that, it will give you an option to debug the program via a Visual Studio dialog where you select the currently running instance of Visual Studio as your debugger instance.&#160; You just remove the statement when you’re done (and you’ll really want to remember to remove it from your production code).&#160; Does the trick, though it is a bit obtrusive to your code base.</p>

  <p>If you haven’t already figured it out, the bug is that my <font face="Courier New">FindAnagramSets</font> method is returning all anagram sets found in the dictionary.&#160; An anagram set of just one word is not really an anagram set, which is why our test specification which is based on that assumption failed.&#160; The fix is to filter out anagram word sets with less than two words before returning, like this:</p>

  <pre class="brush: csharp">// Return anagram sets with more than one word in it
return results.Values.Where(x =&gt; x.Count &gt; 1);</pre>

  <p>That will make the code pass the tests now.&#160; Remember to remove the debugger statement we added.</p>

  <h3>BDD, OOP…WTH?!</h3>

  <p>Just kidding.&#160; But I do want to discuss a reaction I had to BDD after implementing this anagram finding solution.&#160; After I completed the code and finished the tests, I was feeling pretty great.&#160; But then I started thinking about how I didn’t really go down the path that I felt BDD was originally trying to lead me down with <font face="Courier New">AnagramGrouper</font>, which was to create two more dependencies, just as I had done for the first managing class <font face="Courier New">AnagramsFinder</font>.&#160; I mean I felt pretty frustrated because I was digging BDD to this point.&#160; I’m not quite sure what my original expectations were, but I might have thought BDD was magically going to right my algorithms for me as well.</p>

  <p>I decided to discuss the whole thing with <a href="http://adotnetdude.blogspot.com/">Esteban</a> to get some perspective on the issue.&#160; He told me he didn’t know a thing about BDD and how it’s supposed to work, but he figured BDD would lead me to decoupled, cohesive code design, but he couldn’t see how it would aide with algorithmic solutions to the problems at hand.&#160; I’m not sure why I was so naïve, but it makes complete sense.&#160; I mean these methodologies are designed to keep you disciplined to some proven, guiding principles, but you still need to use your brain and creativity to actually solve the problem.&#160; I guess it would help to get some feedback and suggestions from you all in regard to knowing when it is time to switch from BDD architecture design mode to algorithm design mode within the broken down dependencies that handle the very specific sub-responsibilities.&#160; Please let me know if you have found some guiding concept or question you’ve come up with that you put to the test in order to lead you into the right coding mindset for that particular context (design or algorithm; or do I even have the right categories?).</p>

  <p>Perhaps it’s just that this design process feels so new to me.&#160; Maybe I have solely been in the coding, algorithm mindset this whole time throughout my career.&#160; Perhaps it will become more natural with practice and experience.&#160; I guess it just feels like I’m learning to drive a manual, stick-shift car or something, and I am a little rough with the clutch when transitioning between gears.</p>

  <p>To take this discussion further, <a href="http://adotnetdude.blogspot.com/">Esteban</a> made a comment in our conversation about his personal thoughts on these methodologies (like BDD) and Object-Oriented Programming.&#160; He has come to believe that we invent these methodologies like flavors of the week “because as programmers we don’t really get OOP.”&#160; Perhaps OOP just comes more naturally to his thought process that most people (or he admits it’s possible he could just be completely ignorant), but he feels that there’s really just a few simple rules and guidelines to keep in mind at all times in order to keep your objects behaving like real life objects and not acting like data constructs.&#160; I encouraged him to give his theory some additional thought and make it a little more formal by writing it down.&#160; He did this and the following is a link to the resultant blog post:</p>

  <p><a href="http://adotnetdude.blogspot.com/2010/01/3-simple-rules-to-good-object-oriented.html">3 Simple Rules to Good Object Oriented Code</a></p>

  <p>Please read through that post and comment on it.&#160; His idea needs to be criticized and/or validated.&#160; He admits he may have no clue what he’s talking about, but I said that’s why it needs to be put to the test by posting it in the open.&#160; We both think it’s an interesting theory, but we’re unsure if it even holds any water.&#160; Perhaps Esteban’s theory is too simplistic to be practical for many programmers.&#160; Provide feedback and lots of it.</p>

  <h3>Summary</h3>

  <p>So I think I need to wrap this post up about now.&#160; I think I have raised enough discussion points to write a book, but please do take the time to comment on what I’m doing wrong, what I’m doing right, or how to make the BDD mentality more flowing and natural.&#160; I promised to talk about having a Word class as opposed to using primitive strings everywhere, and that will definitely be in the next post.&#160; Also, I think our <font face="Courier New">AnagramGrouper</font> class may need some more testing and flushing out of how it should handle spaces, punctuation, casing, and so on.</p>

  <p>Thanks for sticking with me thus far.</p>
</span>  </div>

]]></content>
  </entry>
  
</feed>
